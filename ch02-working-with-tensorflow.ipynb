{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Working with Tensoflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Objective**: Build a deep learning model that can learn the alphabet.\n",
    "\n",
    "**Agenda**\n",
    "1. Defining the problem\n",
    "2. Visualizing the end model\n",
    "3. Short theroretical recap of LSTM networks\n",
    "4. Implementation\n",
    "   - Encoding the data\n",
    "   - Building the model\n",
    "   - Training\n",
    "5. Using `Tensorboard` to overview model graph and `loss`/`accuracy` evolution\n",
    "   - Structuring the graph with name scopes\n",
    "   - Adding model summaries\n",
    "   - Bonus: debugging data with `text summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Defining the probelm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Although the objective is to **train a deep learning model to learn the alphabet**, this doesn't shed any light on how we can tackle the problem.\n",
    "\n",
    "Since deep learning models have proven to be very good at classification tasks we need to 'reshape' the problem as a classification problem.\n",
    "\n",
    "The new 'shape' of the problem looks like this:\n",
    "\n",
    "> Create a deep learning model that when given a sequence of consecutive letters will output, for each letter of the alphabet, the probability of it being the next letter in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Example**\n",
    "Given `KLM` as input, the model would output something like this:\n",
    "```\n",
    "   Letter  Probability\n",
    "  ---------------------\n",
    "   A        0.07560498\n",
    "   B        0.01971263\n",
    "   C        0.01407314\n",
    "   D        0.00286496\n",
    "   E        0.01043301\n",
    "   F        0.01803329\n",
    "   G        0.03739211\n",
    "   H        0.00691894\n",
    "   I        0.0135167\n",
    "   J        0.08230913\n",
    "   K        0.02166412\n",
    "   L        0.01301833\n",
    "   M        0.00820917\n",
    "   N        0.31165153\n",
    "   O        0.01616119\n",
    "   P        0.05539528\n",
    "   Q        0.00634293\n",
    "   R        0.01654692\n",
    "   S        0.06636301\n",
    "   T        0.00361082\n",
    "   U        0.02698876\n",
    "   V        0.00770648\n",
    "   W        0.07386798\n",
    "   X        0.05238049\n",
    "   Y        0.01679033\n",
    "   Z        0.0224438\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Visualizing the end model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before building a model it's good to have a mental picture of what we want to achieve. In our case we'll be building a model that will receive a sequence of letters and will output a vector of 26 components where each element is the probability of that letter being the next letter in the sequence.\n",
    "\n",
    "![Model architecture](./img/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[LSTM]:Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "ch02-working-with-tensorflow.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
