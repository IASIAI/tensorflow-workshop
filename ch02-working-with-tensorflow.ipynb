{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Working with Tensoflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Objective**: Build a deep learning model that can learn the alphabet.\n",
    "\n",
    "**Agenda**\n",
    "1. Defining the problem\n",
    "2. An overview of the end model\n",
    "3. Short theoretical recap/overview\n",
    "4. Model implementation\n",
    "   - Encoding the data\n",
    "   - Building the model\n",
    "   - Training\n",
    "5. Using `Tensorboard` to overview model graph and `loss`/`accuracy` evolution\n",
    "   - Structuring the graph with name scopes\n",
    "   - Adding model summaries\n",
    "   - Bonus: debugging data with `text summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Defining the probelm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Although the objective is to **train a deep learning model to learn the alphabet**, this doesn't shed any light on how we can tackle the problem.\n",
    "\n",
    "Since deep learning models have proven to be very good at classification tasks we need to 'reshape' the problem as a classification problem.\n",
    "\n",
    "The new 'shape' of the problem looks like this:\n",
    "\n",
    "> Create a deep learning model that when given a sequence of consecutive letters will output, for each letter of the alphabet, the probability of it being the next letter in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Example**\n",
    "Given `KLM` as input, the model would output something like this:\n",
    "```\n",
    "   Letter  Probability\n",
    "  ---------------------\n",
    "   A        0.07560498\n",
    "   B        0.01971263\n",
    "   C        0.01407314\n",
    "   D        0.00286496\n",
    "   E        0.01043301\n",
    "   F        0.01803329\n",
    "   G        0.03739211\n",
    "   H        0.00691894\n",
    "   I        0.0135167\n",
    "   J        0.08230913\n",
    "   K        0.02166412\n",
    "   L        0.01301833\n",
    "   M        0.00820917\n",
    "   N        0.31165153\n",
    "   O        0.01616119\n",
    "   P        0.05539528\n",
    "   Q        0.00634293\n",
    "   R        0.01654692\n",
    "   S        0.06636301\n",
    "   T        0.00361082\n",
    "   U        0.02698876\n",
    "   V        0.00770648\n",
    "   W        0.07386798\n",
    "   X        0.05238049\n",
    "   Y        0.01679033\n",
    "   Z        0.0224438\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## An overview of the end model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before digging into the code it's good to pause and ponder upon the model architecture.\n",
    "\n",
    "Since the model is quite simple our model will have at its core just two deep learning components:\n",
    "1. A **Long Short - Term Memory** (`LSTM`) cell followed by\n",
    "2. A **fully connected** (`dense`) cell\n",
    "\n",
    "Although the above fully define the model we'll be building they don't define the full computational graph. To have a full (and *functional*) computational graph we'll also need the following components:\n",
    "- A **placeholder** node which will feed the input data to the model\n",
    "- Another **placeholder** that will receive the labeled data in training\n",
    "- During training we will need a node to measure the **loss** or *how far away is the predicted output to the expected output*\n",
    "- Also for training we'll want to measure the **accuracy** of the model and subsequently we'll need a node for that in the graph\n",
    "\n",
    "An overview of the graph is in the image below.\n",
    "\n",
    "![Model architecture](./img/model.png)\n",
    "\n",
    "**Remarks**:\n",
    "1. The image contains an additional node `predictions` which practically is the output of the `dense` layer and therefore part of it. However, it's better to keep it as a separate node in order to have a better view of how the data flows through the graph.\n",
    "2. The *real* computational graph contains a lot more nodes but those nodes pertain more to the inner workings of Tensorflow than to our objective so we won't concentrate on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Short theoretical recap/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Theoretical foundations of deep learning are outside of this workshop but we still need some notion of theory in order to build our model. So, let's have a quick dive into it.\n",
    "\n",
    "**Fully connected a. k. a. dense layer**\n",
    "![Dense layer](./img/dense.png)\n",
    "\n",
    "- A fully connected layer is a neural network in which all input nodes are connected to the output/hidden nodes\n",
    "- Each hidden/output node calculates its value as the sum of weighted values of input nodes $h_i=\\sum{x_i\\cdot{w_{i,j}}}$\n",
    "- This operation is nothing else but a **matrix multiplication** of the *weights matrix* $\\pmb{W}$ with the *column vector* $\\pmb{x}$\n",
    "- The *actual* output of the network is the value from the output nodes passed through the *activation function* $y_i=\\sigma{(h_i)}$\n",
    "\n",
    "**Long Short-Term Memory networks**\n",
    "![LSTM network](./img/lstm-chain.png)\n",
    "\n",
    "- Are a special type of Recurrent Neural Networks which are capable of learning long-term dependencies\n",
    "- It does so by passing input through several gates (formulas are here just for fun):\n",
    "\n",
    "  $$\n",
    "    f_t=\\sigma(W_f\\cdot[h_{t-1},x_t]+b_f)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "    i_t=\\sigma(W_i\\cdot[h_{t-1},x_t]+b_i)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "    \\widetilde{C}=tanh(W_C\\cdot[h_{t-1},x_t]+b_C)\n",
    "  $$\n",
    "\n",
    "- Afterwards the cell state is updated:\n",
    "\n",
    "  $$\n",
    "    C_t=f_t*C_{t-1}+i_t*\\widetilde{C}_t\n",
    "  $$\n",
    "\n",
    "- Then the outputs are calculated:\n",
    "  $$\n",
    "    o_t=\\sigma(W_o\\cdot[h_{t-1},x_t]+b_0)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "    h_t=o_t*tanh(C_t)\n",
    "  $$\n",
    "\n",
    "- Although theoretically a Recurrent Neural Network can process sequences of arbitrary length, in practice the network is unrolled into a (parameterized) number of concatenated cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's start with importing the required modules: `numpy` for numeric manipulation and obviously `tensoflow` for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "LSTM cells are defined in `tensorflow.contrib.rnn` so let's import that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We'll also need some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's set the random seed in order to obtain reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And let's define a string to hold the alphabet letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Tensorflow API](https://www.tensorflow.org/api_docs/python/tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "ch02-working-with-tensorflow.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
