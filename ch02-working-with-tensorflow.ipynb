{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Working with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Objective**: Build a deep learning model that can learn the alphabet.\n",
    "\n",
    "**Agenda**\n",
    "1. Defining the problem\n",
    "2. An overview of the end model\n",
    "3. Short theoretical recap/overview\n",
    "4. Model implementation\n",
    "   - Encoding the data\n",
    "   - Generating the training data\n",
    "   - Building the computational graph\n",
    "   - Training\n",
    "5. Using `TensorBoard` to overview model graph and `loss`/`accuracy` evolution\n",
    "   - Structuring the graph with name scopes\n",
    "   - Adding model summaries\n",
    "   - Bonus: debugging data with `text summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Defining the probelm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Although the objective is to **train a deep learning model to learn the alphabet**, this doesn't shed any light on how we can tackle the problem.\n",
    "\n",
    "Since deep learning models have proven to be very good at classification tasks we need to 'reshape' the problem as a classification problem.\n",
    "\n",
    "The new 'shape' of the problem looks like this:\n",
    "\n",
    "> Create a deep learning model that when given a sequence of consecutive letters will output, for each letter of the alphabet, the probability of it being the next letter in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Example**\n",
    "Given `KLM` as input, the model would output something like this:\n",
    "```\n",
    "   Letter  Probability\n",
    "  ---------------------\n",
    "   A        0.07560498\n",
    "   B        0.01971263\n",
    "   C        0.01407314\n",
    "   D        0.00286496\n",
    "   E        0.01043301\n",
    "   F        0.01803329\n",
    "   G        0.03739211\n",
    "   H        0.00691894\n",
    "   I        0.0135167\n",
    "   J        0.08230913\n",
    "   K        0.02166412\n",
    "   L        0.01301833\n",
    "   M        0.00820917\n",
    "   N        0.31165153\n",
    "   O        0.01616119\n",
    "   P        0.05539528\n",
    "   Q        0.00634293\n",
    "   R        0.01654692\n",
    "   S        0.06636301\n",
    "   T        0.00361082\n",
    "   U        0.02698876\n",
    "   V        0.00770648\n",
    "   W        0.07386798\n",
    "   X        0.05238049\n",
    "   Y        0.01679033\n",
    "   Z        0.0224438\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## An overview of the end model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before digging into the code it's good to pause and ponder upon the model architecture.\n",
    "\n",
    "Since the model is quite simple our model will have at its core just two deep learning components:\n",
    "1. A **Long Short - Term Memory** (`LSTM`) cell followed by\n",
    "2. A **fully connected** (`dense`) cell\n",
    "\n",
    "Although the above fully define the model we'll be building they don't define the full computational graph. To have a full (and *functional*) computational graph we'll also need the following components:\n",
    "- A **placeholder** node which will feed the input data to the model\n",
    "- Another **placeholder** that will receive the labeled data in training\n",
    "- During training we will need a node to measure the **loss** or *how far away is the predicted output to the expected output*\n",
    "- Also for training we'll want to measure the **accuracy** of the model and subsequently we'll need a node for that in the graph\n",
    "\n",
    "An overview of the graph is in the image below.\n",
    "\n",
    "![Model architecture](./img/model.png)\n",
    "\n",
    "**Remarks**:\n",
    "1. The image contains an additional node `predictions` which practically is the output of the `dense` layer and therefore part of it. However, it's better to keep it as a separate node in order to have a better view of how the data flows through the graph.\n",
    "2. The *real* computational graph contains a lot more nodes but those nodes pertain more to the inner workings of TensorFlow than to our objective so we won't concentrate on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Short theoretical recap/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Theoretical foundations of deep learning are outside of this workshop but we still need some notion of theory in order to build our model. So, let's have a quick dive into it.\n",
    "\n",
    "**Fully connected a. k. a. dense layer**\n",
    "![Dense layer](./img/dense.png)\n",
    "\n",
    "- A fully connected layer is a neural network in which all input nodes are connected to the output/hidden nodes\n",
    "- Each hidden/output node calculates its value as the sum of weighted values of input nodes $h_i=\\sum{x_i\\cdot{w_{i,j}}}$\n",
    "- This operation is nothing else but a **matrix multiplication** of the *weights matrix* $\\pmb{W}$ with the *column vector* $\\pmb{x}$\n",
    "- The *actual* output of the network is the value from the output nodes passed through the *activation function* $y_i=\\sigma{(h_i)}$\n",
    "\n",
    "**Long Short-Term Memory networks**\n",
    "![LSTM network](./img/lstm-chain.png)\n",
    "\n",
    "- Are a special type of Recurrent Neural Networks which are capable of learning long-term dependencies\n",
    "- It does so by passing input through several gates (formulas are here just for fun):\n",
    "\n",
    "  $$\n",
    "    f_t=\\sigma(W_f\\cdot[h_{t-1},x_t]+b_f)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "    i_t=\\sigma(W_i\\cdot[h_{t-1},x_t]+b_i)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "    \\widetilde{C}=tanh(W_C\\cdot[h_{t-1},x_t]+b_C)\n",
    "  $$\n",
    "\n",
    "- Afterwards the cell state is updated:\n",
    "\n",
    "  $$\n",
    "    C_t=f_t*C_{t-1}+i_t*\\widetilde{C}_t\n",
    "  $$\n",
    "\n",
    "- Then the outputs are calculated:\n",
    "  $$\n",
    "    o_t=\\sigma(W_o\\cdot[h_{t-1},x_t]+b_0)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "    h_t=o_t*tanh(C_t)\n",
    "  $$\n",
    "\n",
    "- Although theoretically a Recurrent Neural Network can process sequences of arbitrary length, in practice the network is unrolled into a (parameterized) number of concatenated cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's start with importing the required modules: `numpy` for numeric manipulation and obviously `tensorflow` for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "LSTM cells are defined in `tensorflow.contrib.rnn` so let's import that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We'll also need some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's set the random seed in order to obtain reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And let's define a string to hold the alphabet letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Since we'll be doing a bunch of matrix multiplications we cannot send to our model a sequence of letters; thus we need to encode the sequence into a numerical form.\n",
    "\n",
    "To do so, we'll take the easiest approach and just assign each letter a zero-based index.\n",
    "\n",
    "We'll create a class that holds the mappings between each letter and it's index and the mappings between each index and its associated letter.\n",
    "\n",
    "Afterwards we'll create two methods:\n",
    "- `encode` will receive a sequence or a single letter and will output the sequence of corresponding indices or the corresponding index respecively\n",
    "- `decode` will receive the predicted index and will return the corresponding letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Encoding():\n",
    "    def __init__(self):\n",
    "        self._char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "        self._int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        return [self._char_to_int[char] for char in sequence]\n",
    "\n",
    "    def encode_letter(self, letter):\n",
    "        return self._char_to_int[letter]\n",
    "\n",
    "    def decode_letter(self, value):\n",
    "        return self._int_to_char[value]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's do a small test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "e = Encoding()\n",
    "print('DEF->{}'.format(e.encode_sequence('DEF')))\n",
    "print('A->{}'.format(e.encode_letter('A')))\n",
    "print('24->{}'.format(e.decode_letter(24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Generating the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before building a model we need to have a data set on which to train the model.\n",
    "\n",
    "Again, since the model is quite simple we can just generate random sequences of consecutive letters as the input data and grab the next letter in the alphabet as the label.\n",
    "\n",
    "To do so, we'll define a new class called `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, size=1000, seq_length=5, print_data=True):\n",
    "        self._size = size\n",
    "        self._sequence_length = seq_length\n",
    "        self._inputs = []\n",
    "        self._labels = []\n",
    "        self._print_data = print_data\n",
    "        self._encoding = Encoding()\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(alphabet)\n",
    "\n",
    "    @property\n",
    "    def sequence_length(self):\n",
    "        return self._sequence_length\n",
    "\n",
    "    def initialize(self):\n",
    "        self._generate_random_data()\n",
    "        self._reshape_inputs()\n",
    "        self._normalize_inputs()\n",
    "        self._normalize_labels()\n",
    "\n",
    "    def shuffle(self):\n",
    "        perm = np.arange(self._size)\n",
    "        np.random.shuffle(perm)\n",
    "        self._inputs = self._inputs[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "\n",
    "\n",
    "    def _normalize_labels(self):\n",
    "        self._labels = to_categorical(self._labels, num_classes=self.num_classes)\n",
    "        self._labels = np.reshape(self._labels, (self._size, 1, self.num_classes))\n",
    "\n",
    "    def _reshape_inputs(self):\n",
    "        self._inputs = pad_sequences(self._inputs,\n",
    "                                     maxlen=self._sequence_length,\n",
    "                                     dtype='float32')\n",
    "        self._inputs = np.reshape(self._inputs, (self._size, self._sequence_length, 1))\n",
    "\n",
    "    def _normalize_inputs(self):\n",
    "        self._inputs = self._inputs / float(self.num_classes)\n",
    "\n",
    "    def _generate_random_data(self):\n",
    "        for i in range(self._size):\n",
    "            start = np.random.randint(self.num_classes - 2)\n",
    "            end = np.random.randint(start, min(start + self._sequence_length, self.num_classes - 1))\n",
    "            input_seq = alphabet[start:end + 1]\n",
    "            output_seq = alphabet[end + 1]\n",
    "\n",
    "            if(self._print_data):\n",
    "                print(\"{}->{}\".format(input_seq, output_seq))\n",
    "\n",
    "            sample = self._encoding.encode_sequence(input_seq)\n",
    "            label = self._encoding.encode_letter(output_seq)\n",
    "\n",
    "            self._inputs.append(sample)\n",
    "            self._labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In God we trust; the rest we test. Let's see if our `Dataset` class behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(size=5)\n",
    "dataset.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Building the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To build the computational graph of the model we just have to link toghether the operations on tensors.\n",
    "\n",
    "Each graph has one or more entry points into which the data is fed. In our case we need to define two of these points: one for the input data and another one for the labels of the training samples.\n",
    "\n",
    "To define our entry points we'll declare two [placeholders](https://www.tensorflow.org/api_docs/python/tf/placeholder) upon which we'll subsequently build our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float32', shape=(dataset.sequence_length, 1), name='inputs')\n",
    "y = tf.placeholder('float32', shape=(1, dataset.num_classes), name=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before creating the LSTM node we need to make sure it will receive input of proper shape. The proper shape in this case is a list of scalars. We can reshape the input tensor with [`tf.split`](https://www.tensorflow.org/api_docs/python/tf/split) operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x1 = tf.split(x, dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next, we'll build the LSTM node. As stated before, LSTM networks are quite heavy on computational resources thus we cannot have infinite sequence length. We'll just link together 5 units to accomodate the maximum length of the sequences from the dataset.\n",
    "\n",
    "We need to specify the number of units of the LSTM cell; normally this would be a hyperparameter but for now we can leave it at 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "inner_cells = [rnn.BasicLSTMCell(num_units=32) for _ in range(dataset.sequence_length)]\n",
    "rnn_cell = rnn.MultiRNNCell(inner_cells)\n",
    "outputs, states = rnn.static_rnn(rnn_cell, x1, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now that we have the outputs of the LSTM layer (we're not interested in intermediary states) we need to take the last output and reshape it to a tensor of shape `[1, 32]`.\n",
    "\n",
    "To do so, we employ [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) to get the last output and [`tf.reshape`](https://www.tensorflow.org/api_docs/python/tf/reshape) to reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "output = tf.nn.embedding_lookup(outputs, dataset.sequence_length - 1)\n",
    "output = tf.reshape(output, [1, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Not really necessary in our case but for demonstrative purposes let's implement the famous equation\n",
    "$$\n",
    "\\hat{\\pmb{y}}=\\pmb{W}\\pmb{x}+\\pmb{b}\n",
    "$$\n",
    "\n",
    "To do so we need to declare the square matrix $\\pmb{W}$ and bias vector $\\pmb{b}$ as two variables initialized with random values from a normal probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([32, 32]), name=\"W\")\n",
    "b = tf.Variable(tf.random_normal([1, 32]), name=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now, the output becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "output = tf.matmul(output, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we can link the output of the LSTM layer to a dense layer created with [`tf.layers.dense`](https://www.tensorflow.org/api_docs/python/tf/layers/Dense):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dense = tf.layers.dense(inputs=output, units=dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To get the class probabilities from the output of the `dense` layer we need to apply [`tf.nn.softmax`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to it and to get the pedicted letter we apply [`tf.argmax`](https://www.tensorflow.org/api_docs/python/tf/argmax) which returns the index of the letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "probs = tf.nn.softmax(dense)\n",
    "pred = tf.argmax(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is how the data will flow through the graph. But to get meaningfull and correct results from the graph we need to add the nodes which will be responsible for minimizing the loss and measuring accuracy during training.\n",
    "\n",
    "To minimize the loss, we'll define cost node using [`tf.reduce_mean`](https://www.tensorflow.org/api_docs/python/tf/reduce_mean) and [`tf.nn.softmax_cross_entropy_with_logits_v2`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2). Afterwards we'll create an `optimizer` node ([`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) in our case) to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=dense, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To measure the accuracy we create new nodes using [`tf.metrics.accuracy`](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) to which we pass the predictions and the expected index from the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "accuracy, accuracy_update = tf.metrics.accuracy(labels=tf.argmax(y, 1), predictions=pred, name='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The model is ready to be trained now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before training the model, let's generate some proper amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(size=1000, print_data=False)\n",
    "dataset.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To train the model we must initialize global and local variables first (`accuracy_update` is a local variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init_locals = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Then we'll proceed according to the drill - we will initialize a session, run the variable initializers and train the model for 500 epochs (another hyperparameter).\n",
    "\n",
    "At each epoch we will shuffle the dataset and will feed the whole dataset to the optimizer. Afterwards we'll measure the accuracy and loss for the whole dataset and print the last values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(init)\n",
    "session.run(init_locals)\n",
    "epoch = 0\n",
    "while epoch < 500:\n",
    "    print(\"Training epoch {:<4d}\".format(epoch), end='\\t')\n",
    "    dataset.shuffle()\n",
    "    # Train the model\n",
    "    for instance, label in zip(dataset.inputs, dataset.labels):\n",
    "        session.run(optimizer, feed_dict={x: instance,\n",
    "                                          y: label})\n",
    "        # Calculate accuracy and loss\n",
    "    for instance, label in zip(dataset.inputs, dataset.labels):\n",
    "        acc, update_op, loss = session.run([accuracy, accuracy_update, cost],\n",
    "                                           feed_dict={x: instance,\n",
    "                                                      y: label})\n",
    "    print(\"Accuracy: {:.8f} \\tLoss: {:.8f}\".format(acc, loss))\n",
    "    epoch = epoch + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now, let's put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "seq = input('Enter a sequence of max 5 consecutive letters:')\n",
    "seq = seq.upper()\n",
    "print(\"You entered {}\".format(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Once we have the input sequence, we need to encode, reshape and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "seq = e.encode_sequence(seq)\n",
    "seq = pad_sequences([seq], dataset.sequence_length)\n",
    "seq = np.reshape(seq, (dataset.sequence_length, 1))\n",
    "seq = seq / float(dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Then, send it to the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "result = session.run(pred, feed_dict={x: seq})\n",
    "letter = result[0]\n",
    "print(\"The next letter is: {}\".format(e.decode_letter(letter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Finally, close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [TensorFlow API](https://www.tensorflow.org/api_docs/python/tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "ch02-working-with-tensorflow.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
