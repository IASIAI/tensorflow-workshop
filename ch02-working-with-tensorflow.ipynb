{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Working with Tensoflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Objective**: Build a deep learning model that can learn the alphabet.\n",
    "\n",
    "**Agenda**\n",
    "1. Defining the problem\n",
    "2. An overview of the end model\n",
    "3. Short theoretical recap/overview\n",
    "4. Implementation\n",
    "   - Encoding the data\n",
    "   - Building the model\n",
    "   - Training\n",
    "5. Using `Tensorboard` to overview model graph and `loss`/`accuracy` evolution\n",
    "   - Structuring the graph with name scopes\n",
    "   - Adding model summaries\n",
    "   - Bonus: debugging data with `text summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Defining the probelm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Although the objective is to **train a deep learning model to learn the alphabet**, this doesn't shed any light on how we can tackle the problem.\n",
    "\n",
    "Since deep learning models have proven to be very good at classification tasks we need to 'reshape' the problem as a classification problem.\n",
    "\n",
    "The new 'shape' of the problem looks like this:\n",
    "\n",
    "> Create a deep learning model that when given a sequence of consecutive letters will output, for each letter of the alphabet, the probability of it being the next letter in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Example**\n",
    "Given `KLM` as input, the model would output something like this:\n",
    "```\n",
    "   Letter  Probability\n",
    "  ---------------------\n",
    "   A        0.07560498\n",
    "   B        0.01971263\n",
    "   C        0.01407314\n",
    "   D        0.00286496\n",
    "   E        0.01043301\n",
    "   F        0.01803329\n",
    "   G        0.03739211\n",
    "   H        0.00691894\n",
    "   I        0.0135167\n",
    "   J        0.08230913\n",
    "   K        0.02166412\n",
    "   L        0.01301833\n",
    "   M        0.00820917\n",
    "   N        0.31165153\n",
    "   O        0.01616119\n",
    "   P        0.05539528\n",
    "   Q        0.00634293\n",
    "   R        0.01654692\n",
    "   S        0.06636301\n",
    "   T        0.00361082\n",
    "   U        0.02698876\n",
    "   V        0.00770648\n",
    "   W        0.07386798\n",
    "   X        0.05238049\n",
    "   Y        0.01679033\n",
    "   Z        0.0224438\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## An overview of the end model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before digging into the code it's good to pause and ponder upon the model architecture.\n",
    "\n",
    "Since the model is quite simple our model will have at its core just two deep learning components:\n",
    "1. A **Long Short - Term Memory** (`LSTM`) cell followed by\n",
    "2. A **fully connected** (`dense`) cell\n",
    "\n",
    "Although the above fully define the model we'll be building they don't define the full computational graph. To have a full (and *functional*) computational graph we'll also need the following components:\n",
    "- A **placeholder** node which will feed the input data to the model\n",
    "- Another **placeholder** that will receive the labeled data in training\n",
    "- During training we will need a node to measure the **loss** or *how far away is the predicted output to the expected output*\n",
    "- Also for training we'll want to measure the **accuracy** of the model and subsequently we'll need a node for that in the graph\n",
    "\n",
    "An overview of the graph is in the image below.\n",
    "\n",
    "![Model architecture](./img/model.png)\n",
    "\n",
    "**Remarks**:\n",
    "1. The image contains an additional node `predictions` which practically is the output of the `dense` layer and therefore part of it. However, it's better to keep it as a separate node in order to have a better view of how the data flows through the graph.\n",
    "2. The *real* computational graph contains a lot more nodes but those nodes pertain more to the inner workings of Tensorflow than to our objective so we won't concentrate on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Short theoretical recap/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[LSTM]:Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "ch02-working-with-tensorflow.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
